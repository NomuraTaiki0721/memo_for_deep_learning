#深層学習まとめ
深層学習で調べたことをまとめる
##基本
形式ニューロンモデルのノードの重みを教師データの出力との誤差が小さくなるように学習させるものがニューラルネットワークである。
Hintonが2006年に発表したボルツマンマシンがベース。
*誤差逆伝播法*や*long short term memory法*により学習。
活性化関数にはシグモイド関数などが用いられる。
###内部表現
*特徴*や*潜在表現*とも呼ばれる。層が多いNNによって観測データから本質的な情報を抽出したもの。
次元削減ともいい、入力情報から認識に必要な低次元の情報を取り出し、その情報を用いて認識を行うことは有効である場合が多い。
*ノーフリーランチ定理no free lunch thorem*ではすべてのタスクに対して他より優れた性能を示す万能アルゴリズムが存在しないことを証明している。
良い内部表現自体を入力データから学習させることを*表現学習*という。

情報量、独立性、説明性、スパーズ性(非0の値が少ないこと)、不変性、ロバスト性、平滑性等が重要。

次元圧縮による内部表現の学習法には主成分分析(PCA)や因子分析(FA)、独立成分分析(ICA)などが使われる。





##深層ニューラルネット
狭義の意味では4層以上の階層型NNのことを指す。
3層でも中間層のノードが十分なら任意の関数を近似できるという理論的な利点があったが、局所最適解や勾配消去問題等の技術的な問題があった。
中間層の数を増やすことで効果的に学習ができることが判明し、活性化関数の工夫がされたことにより、2010年代に普及した。
##再帰ニューラルネット


##畳み込みニューラルネット
畳み込み構造を含んだNN、画像処理特に使われる。


##再帰結合ニューラルネット
系列データ処理のために作られたNN、前回時刻の入力の情報を現在の入力に使うための再帰結合入力がある。
音声認識、自然言語処理で使われる。



##画像処理分野



##音声処理分野



##各種ツール
###Cafee
画像処理で有名。昔からある。学習済みNNを配布してる。
###Chainer
pythonでよく使われる。汎用性が高いらしい。
###Google Tensorflow
モデルの可視化が可能なフレームワーク
###Chainer gogh
画像の合成のためのツール。おもしろい。
###Pylearn2
よくわからん。
###Imagenet
画像集








